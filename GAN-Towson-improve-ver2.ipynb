{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Running Import Statements and ensuring GPU Support"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16bc02c3fb613f07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:34:15.490942Z",
     "start_time": "2024-04-16T21:34:12.731864Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices and configure them before any other operations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth on the GPU to true\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth set\")\n",
    "            print(\"GPU Device:\", gpu, \"\\n\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before initializing the GPUs\n",
    "        print(\"RuntimeError in setting up GPU:\", e)\n",
    "        \n",
    "    try:\n",
    "        # Optional: Set a memory limit\n",
    "        memory_limit = 8000  # e.g., 4096 MB for 4GB\n",
    "        config = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [config])\n",
    "        print(f\"Memory limit set to {memory_limit}MB on GPU {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory limit: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Print versions and device configurations after ensuring GPU settings\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA version:\", tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(\"cuDNN version:\", tf.sysconfig.get_build_info()['cudnn_version'])\n",
    "print(tf.config.list_physical_devices(), \"\\n\", tf.config.list_logical_devices(), \"\\n\")\n",
    "print(tf.config.list_physical_devices('GPU'), \"\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "Loading the Dataset",
   "metadata": {
    "collapsed": false
   },
   "id": "6dbd3b980448fb21"
  },
  {
   "cell_type": "code",
   "source": [
    "rel_path = '/archive/'          # If your dataset is within your python project directory, change this to the relative path to your dataset\n",
    "path = os.getcwd() + rel_path   # If your dataset is somewhere else, change this to that path\n",
    "csv_filepaths = glob.glob(os.path.join(path, \"*.csv\"))  # Makes a list of all CSVs within the directory above\n",
    "\n",
    "csv_filepaths = csv_filepaths[:40]\n",
    "\n",
    "# Load the first csv file\n",
    "df = pd.read_csv(csv_filepaths[0]) #astype(column_datatypes)\n",
    "\n",
    "# Load csv files in 10-file batches \n",
    "batch_size = 10\n",
    "\n",
    "for i in range(1, len(csv_filepaths)):\n",
    "    clear_output(wait=False) # Pretty output\n",
    "    print(f'Loading CSV {i}')\n",
    "    \n",
    "    # First file of each batch, restart the batch list\n",
    "    if i % batch_size == 1:\n",
    "        batch = [df]\n",
    "    \n",
    "    batch.append(pd.read_csv(csv_filepaths[i])) #astype(column_datatypes))    # Load a CSV and change relevant columns to bools\n",
    "    \n",
    "    # every #batch_size# file, add it to the df dataframe\n",
    "    if i % batch_size == 0:\n",
    "        df = pd.concat(batch)\n",
    "        batch.clear()   # Get rid of old batch files to free memory\n",
    "        print(f'Loaded to {i}')\n",
    "\n",
    "# Load any remaining data in batch\n",
    "if len(batch) != 0:\n",
    "    print(\"Loading data from final batch.\")\n",
    "    df = pd.concat(batch)\n",
    "\n",
    "clear_output(wait=False)\n",
    "del batch\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6107541190bd38e",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        flow_duration  Header_Length  Protocol Type  Duration          Rate  \\\n",
       "0            0.000000          54.00           6.00     64.00      0.329807   \n",
       "1            0.000000          57.04           6.33     64.00      4.290556   \n",
       "2            0.000000           0.00           1.00     64.00     33.396799   \n",
       "3            0.328175       76175.00          17.00     64.00   4642.133010   \n",
       "4            0.117320         101.73           6.11     65.91      6.202211   \n",
       "...               ...            ...            ...       ...           ...   \n",
       "437357       0.086171       31001.00          17.00     64.00   8560.772402   \n",
       "437358       0.000000           0.00          46.53     63.36      2.591956   \n",
       "437359       5.636653         108.00           6.00     64.00      0.354820   \n",
       "437360       0.000000          54.00           6.00     64.00     18.172690   \n",
       "437361       0.024897       16025.00          17.00     64.00  12864.464043   \n",
       "\n",
       "               Srate  Drate  fin_flag_number  syn_flag_number  \\\n",
       "0           0.329807    0.0              1.0              0.0   \n",
       "1           4.290556    0.0              0.0              0.0   \n",
       "2          33.396799    0.0              0.0              0.0   \n",
       "3        4642.133010    0.0              0.0              0.0   \n",
       "4           6.202211    0.0              0.0              1.0   \n",
       "...              ...    ...              ...              ...   \n",
       "437357   8560.772402    0.0              0.0              0.0   \n",
       "437358      2.591956    0.0              0.0              0.0   \n",
       "437359      0.354820    0.0              0.0              1.0   \n",
       "437360     18.172690    0.0              0.0              0.0   \n",
       "437361  12864.464043    0.0              0.0              0.0   \n",
       "\n",
       "        rst_flag_number  ...        Std  Tot size           IAT  Number  \\\n",
       "0                   1.0  ...   0.000000     54.00  8.334383e+07     9.5   \n",
       "1                   0.0  ...   2.822973     57.04  8.292607e+07     9.5   \n",
       "2                   0.0  ...   0.000000     42.00  8.312799e+07     9.5   \n",
       "3                   0.0  ...   0.000000     50.00  8.301570e+07     9.5   \n",
       "4                   0.0  ...  23.113111     57.88  8.297300e+07     9.5   \n",
       "...                 ...  ...        ...       ...           ...     ...   \n",
       "437357              0.0  ...   0.000000     50.00  8.312382e+07     9.5   \n",
       "437358              0.0  ...   8.802696    586.68  8.368106e+07     9.5   \n",
       "437359              0.0  ...   0.000000     54.00  8.298588e+07     9.5   \n",
       "437360              0.0  ...   0.000000     54.00  8.306725e+07     9.5   \n",
       "437361              0.0  ...   0.000000     50.00  8.301542e+07     9.5   \n",
       "\n",
       "         Magnitue     Radius   Covariance  Variance  Weight  \\\n",
       "0       10.392305   0.000000     0.000000      0.00  141.55   \n",
       "1       10.464666   4.010353   160.987842      0.05  141.55   \n",
       "2        9.165151   0.000000     0.000000      0.00  141.55   \n",
       "3       10.000000   0.000000     0.000000      0.00  141.55   \n",
       "4       11.346876  32.716243  3016.808286      0.19  141.55   \n",
       "...           ...        ...          ...       ...     ...   \n",
       "437357  10.000000   0.000000     0.000000      0.00  141.55   \n",
       "437358  34.343416  12.489157  1117.089932      0.07  141.55   \n",
       "437359  10.392305   0.000000     0.000000      0.00  141.55   \n",
       "437360  10.392305   0.000000     0.000000      0.00  141.55   \n",
       "437361  10.000000   0.000000     0.000000      0.00  141.55   \n",
       "\n",
       "                     label  \n",
       "0         DDoS-RSTFINFlood  \n",
       "1            DoS-TCP_Flood  \n",
       "2          DDoS-ICMP_Flood  \n",
       "3            DoS-UDP_Flood  \n",
       "4            DoS-SYN_Flood  \n",
       "...                    ...  \n",
       "437357      DDoS-UDP_Flood  \n",
       "437358  Mirai-greeth_flood  \n",
       "437359       DoS-SYN_Flood  \n",
       "437360      DDoS-TCP_Flood  \n",
       "437361       DoS-UDP_Flood  \n",
       "\n",
       "[10287724 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.329807</td>\n",
       "      <td>0.329807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>8.334383e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.392305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DDoS-RSTFINFlood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.04</td>\n",
       "      <td>6.33</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4.290556</td>\n",
       "      <td>4.290556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.822973</td>\n",
       "      <td>57.04</td>\n",
       "      <td>8.292607e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.464666</td>\n",
       "      <td>4.010353</td>\n",
       "      <td>160.987842</td>\n",
       "      <td>0.05</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-TCP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>33.396799</td>\n",
       "      <td>33.396799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.312799e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DDoS-ICMP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328175</td>\n",
       "      <td>76175.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4642.133010</td>\n",
       "      <td>4642.133010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.301570e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-UDP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117320</td>\n",
       "      <td>101.73</td>\n",
       "      <td>6.11</td>\n",
       "      <td>65.91</td>\n",
       "      <td>6.202211</td>\n",
       "      <td>6.202211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.113111</td>\n",
       "      <td>57.88</td>\n",
       "      <td>8.297300e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.346876</td>\n",
       "      <td>32.716243</td>\n",
       "      <td>3016.808286</td>\n",
       "      <td>0.19</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-SYN_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437357</th>\n",
       "      <td>0.086171</td>\n",
       "      <td>31001.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>8560.772402</td>\n",
       "      <td>8560.772402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.312382e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DDoS-UDP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437358</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.53</td>\n",
       "      <td>63.36</td>\n",
       "      <td>2.591956</td>\n",
       "      <td>2.591956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.802696</td>\n",
       "      <td>586.68</td>\n",
       "      <td>8.368106e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>34.343416</td>\n",
       "      <td>12.489157</td>\n",
       "      <td>1117.089932</td>\n",
       "      <td>0.07</td>\n",
       "      <td>141.55</td>\n",
       "      <td>Mirai-greeth_flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437359</th>\n",
       "      <td>5.636653</td>\n",
       "      <td>108.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.354820</td>\n",
       "      <td>0.354820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>8.298588e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.392305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-SYN_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437360</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>18.172690</td>\n",
       "      <td>18.172690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>8.306725e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.392305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DDoS-TCP_Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437361</th>\n",
       "      <td>0.024897</td>\n",
       "      <td>16025.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>12864.464043</td>\n",
       "      <td>12864.464043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.301542e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>DoS-UDP_Flood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10287724 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataframe Memory Size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d196c76ad9f33c54"
  },
  {
   "cell_type": "code",
   "source": [
    "tot_mem = df.memory_usage().sum()\n",
    "print(f'{tot_mem / 1000000000} gb')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:35:03.841837Z",
     "start_time": "2024-04-16T21:35:03.827821Z"
    }
   },
   "id": "22923238d1d94197",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.950486016 gb\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64aaa3e725257871"
  },
  {
   "cell_type": "code",
   "source": [
    "label_maps = { 'Backdoor_Malware': 0,         'BenignTraffic': 1,           'BrowserHijacking': 2,\n",
    "               'CommandInjection': 3,         'DDoS-ACK_Fragmentation': 4,  'DDoS-HTTP_Flood': 5,\n",
    "               'DDoS-ICMP_Flood': 6,          'DDoS-ICMP_Fragmentation': 7, 'DDoS-PSHACK_Flood': 8,\n",
    "               'DDoS-RSTFINFlood': 9,         'DDoS-SYN_Flood': 10,         'DDoS-SlowLoris': 11,\n",
    "               'DDoS-SynonymousIP_Flood': 12, 'DDoS-TCP_Flood': 13,         'DDoS-UDP_Flood': 14,\n",
    "               'DDoS-UDP_Fragmentation': 15,  'DNS_Spoofing': 16,           'DictionaryBruteForce': 17,\n",
    "               'DoS-HTTP_Flood': 18,          'DoS-SYN_Flood': 19,          'DoS-TCP_Flood': 20,\n",
    "               'DoS-UDP_Flood': 21,           'MITM-ArpSpoofing': 22,       'Mirai-greeth_flood': 23,\n",
    "               'Mirai-greip_flood': 24,       'Mirai-udpplain': 25,         'Recon-HostDiscovery': 26,\n",
    "               'Recon-OSScan': 27,            'Recon-PingSweep': 28,        'Recon-PortScan': 29,\n",
    "               'SqlInjection': 30,            'Uploading_Attack': 31,       'VulnerabilityScan': 32, \n",
    "               'XSS': 33\n",
    "             }\n",
    "\n",
    "df['label'] = df['label'].map(label_maps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:35:09.320886Z",
     "start_time": "2024-04-16T21:35:08.802738Z"
    }
   },
   "id": "5c4230b84755fee9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a63563bc6cf116"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e2bd6be73552dd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters for the Machine Learning Model or GAN setup\n",
    "\n",
    "# Input shape for the model or the initial layer of the generator\n",
    "input_shape = 46\n",
    "\n",
    "# Training Configuration\n",
    "# num_epochs = 50       # Number of training epochs overall (if applicable)\n",
    "batch_size = 1024     # Batch size for training\n",
    "epochs = 7000        # Specific to the generator or another component\n",
    "\n",
    "# GAN-specific Parameters\n",
    "critic_updates = 5   # Number of critic updates per generator update in a GAN\n",
    "\n",
    "# Sampling and Class Configuration\n",
    "num_samples = 10000    # Number of samples to generate or process\n",
    "specific_attack_classes = [0, 1, 2, 3, 4 , 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "num_classes = len(specific_attack_classes)  # Total number of unique classes\n",
    "\n",
    "# Display DataFrame (Optional: you can remove this if it was for a check)\n",
    "result = df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:35:32.766256Z",
     "start_time": "2024-04-16T21:35:32.761254Z"
    }
   },
   "id": "51af0bb4842459d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Towson Normal GAN Structure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b3b7de14e652cc"
  },
  {
   "cell_type": "code",
   "source": [
    "# GAN class\n",
    "# This class contains the generator and discriminator models, as well as the training loop for the GAN\n",
    "class GAN:\n",
    "    def __init__(self, hidden1, hidden2, hidden3, input_shape, num_classes):\n",
    "        # store the parameters as instance variables\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.hidden3 = hidden3\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # build the generator and discriminator\n",
    "        self.generator = self.build_generator(self.hidden1, self.hidden2, self.hidden3, self.input_shape)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # setting the loss function for generator and discriminator\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "        # self.generator.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "        self.discriminator.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def build_generator(self, hidden1, hidden2, hidden3, input_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden1, input_dim=input_dim))  \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden3))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(input_dim, activation='relu'))  # Changed from output_dim to input_dim\n",
    "\n",
    "        noise = Input(shape=(input_dim,))\n",
    "        attack = model(noise)\n",
    "        return Model(noise, attack)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_shape, input_dim=input_shape, activation='relu'))  \n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(15, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "        attack = Input(shape=(input_shape,))\n",
    "        validity = model(attack)\n",
    "\n",
    "        return Model(attack, validity)\n",
    "    \n",
    "   \n",
    "    # def discriminator_loss(self, real_output, fake_output):\n",
    "    #     return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "    # \n",
    "    # def generator_loss(self, fake_output):\n",
    "    #     return tf.reduce_mean(fake_output)\n",
    "\n",
    "\n",
    "    def trainGAN(self, gen_hidden1, gen_hidden2, gen_hidden3, input_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param gen_hidden1: \n",
    "        :param gen_hidden2: \n",
    "        :param gen_hidden3: \n",
    "        :param input_dim: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Setting up Optimizer\n",
    "        \"\"\"\n",
    "        # optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        \"\"\"\n",
    "        Getting the data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Directly use 'result' DataFrame. Ensure it's accessible within this scope.\n",
    "        # Sampling 500 data points randomly from 'result'\n",
    "        sampled_df = result.sample(10000).reset_index(drop=True)\n",
    "        \"\"\"\n",
    "        Redundant Label Encoding\n",
    "        \"\"\"\n",
    "        le = LabelEncoder()\n",
    "        sampled_df['label'] = le.fit_transform(sampled_df['label'])\n",
    "        \n",
    "        \"\"\"\n",
    "        Splitting the data into features and labels\n",
    "        \"\"\"\n",
    "        # Split the data into training and testing sets\n",
    "        X = sampled_df.drop('label', axis=1)  # Features\n",
    "        y = sampled_df['label']               # Target label\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Ensure you reset index on X_train if accessing by loc or iloc later\n",
    "        X_train.reset_index(drop=True, inplace=True)\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        y_train.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Save the test data for later\n",
    "        X_test.to_csv('X_test.csv', index=False)  # Save X_test data to CSV\n",
    "        y_test.to_csv('y_test.csv', index=False)  # Save y_test data to CSV\n",
    "        \n",
    "        # Output the memory usage and the shapes of training/testing datasets\n",
    "        print(f'Memory usage: {df.memory_usage().sum() / 1000000000} GB')\n",
    "        print('Training set shape:', X_train.shape)\n",
    "        print('Test set shape:', X_test.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        Setting up labels for valid (real) and fake data for training\n",
    "        \"\"\"\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        \"\"\"\n",
    "        Building the discriminator\n",
    "        \"\"\"\n",
    "        # discriminator = self.build_discriminator()\n",
    "        # discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        \"\"\"\n",
    "        Building the generator\n",
    "        \"\"\"\n",
    "        # generator = self.build_generator(gen_hidden1, gen_hidden2, gen_hidden3, input_dim)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Setting up the combined model\n",
    "        \"\"\"\n",
    "        z = Input(shape=(input_shape,))\n",
    "        attack = self.generator(z)\n",
    "        validity = self.discriminator(attack)\n",
    "        combined = Model(z, validity)\n",
    "        combined.compile(loss='categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \"\"\"\n",
    "        set up of break conditions for training when the generator is worsening\n",
    "        \"\"\"\n",
    "        loss_increase_count = 0\n",
    "        prev_g_loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        TRAINING LOOP\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Get Training Data from X_train\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_attacks = X_train.iloc[idx]\n",
    "            \n",
    "            # Ensure all data is numeric and replace NaNs\n",
    "            real_attacks = real_attacks.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "        \n",
    "            # Convert DataFrame to NumPy array and ensure dtype is float32\n",
    "            real_attacks_np = real_attacks.astype('float32').to_numpy()  # is it normalized?\n",
    "\n",
    "            # Run Generator\n",
    "            noise = tf.random.normal((batch_size, input_shape))  # Make the noise input\n",
    "            gen_attacks = self.generator.predict(noise)  # Make the synthetic data\n",
    "            \n",
    "            # Train Discriminator with training data and synthetic data\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_attacks_np, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_attacks, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator from the noise and valid label by using the combined model to have the generator interact with the discriminator\n",
    "            g_loss = combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            # at the end of 100 epochs print the losses and accuracy\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "            \n",
    "            # if the loss is greater than previous loss then increase the counter \n",
    "            if (g_loss - prev_g_loss) > 0: \n",
    "                loss_increase_count = loss_increase_count + 1\n",
    "            else: \n",
    "                loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "                \n",
    "            prev_g_loss = g_loss\n",
    "            \n",
    "            # Conditions to stop the loop if generator loss increases 5 times    \n",
    "            if loss_increase_count > 5:\n",
    "                print('Stoping on iteration: ', epoch)\n",
    "                break\n",
    "            \n",
    "            # saving the generated output\n",
    "            if epoch % 20 == 0:\n",
    "                f = open(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", \"a\")\n",
    "                np.savetxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "                f.close()\n",
    "\n",
    "        # peek at our results\n",
    "        results = np.loadtxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\")  # save final output\n",
    "        print(\"Generated attacks: \")\n",
    "        print(results[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T21:35:39.605024Z",
     "start_time": "2024-04-16T21:35:39.589989Z"
    }
   },
   "id": "e440bc51ca32170f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GAN Setup & Training Prep"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26106e7d66f68152"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Randomly select hidden layer sizes for the generator\n",
    "gen_hidden1 = np.random.randint(1, 101)\n",
    "gen_hidden2 = np.random.randint(1, 101)\n",
    "gen_hidden3 = np.random.randint(1, 101)\n",
    "\n",
    "# Create the GAN with the selected hidden layer sizes\n",
    "gan = GAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape, num_classes)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Hidden Layers: \", gen_hidden1, gen_hidden2, gen_hidden3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b6150bc0837309",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layers:  93 41 26\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN GAN Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd6995f4eee9f52c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the trainGAN function directly to start training\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "gan.trainGAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "clear_output(wait=False)\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "print(\"Training Complete in {:.2f} seconds!!!\".format(end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38ec80b1a28cac2",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAN with hidden layers:  93 41 26\n",
      "Training Complete in 686.60 seconds!!!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f19b5928dd91b7"
  },
  {
   "cell_type": "code",
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    # define baseline model\n",
    "    def baseline_model(self, num_of_classes):\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        inputs = 46\n",
    "        hidden_layer1 = 10\n",
    "        hidden_layer2 = 5\n",
    "        hidden_layer3 = 0\n",
    "        outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "        \n",
    "        model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "        if hidden_layer2 != 0:\n",
    "            model.add(Dense(hidden_layer2, activation='relu'))\n",
    "        if hidden_layer3 != 0:\n",
    "            model.add(Dense(hidden_layer3, activation='relu'))\n",
    "        model.add(Dense(outputs, activation='softmax'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #optimizer=adam\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6995ec92cf76edf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#for i in range(0,10):\n",
    "\n",
    "# set up seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# import test data\n",
    "X = pd.read_csv('X_test.csv')  # Load X_test data from CSV\n",
    "Y = pd.read_csv('y_test.csv')  # Load y_test data from CSV\n",
    "\n",
    "# set up for Y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# set up evaluator \n",
    "evaluatorModel = Evaluator()\n",
    "\n",
    "estimator = KerasClassifier(build_fn=evaluatorModel.baseline_model(34), epochs=32, batch_size=200, verbose=2)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "y_pred = cross_val_predict(estimator, X, dummy_y, cv=kfold)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "\n",
    "trained_classifier = estimator.fit(X, Y)\n",
    "print(type(estimator))\n",
    "\n",
    "cm = confusion_matrix(Y, y_pred)\n",
    "print(cm)\n",
    "print(\"total: \" + str(cm.sum()))\n",
    "print(\"accuracy: \" + str(np.trace(cm) / cm.sum()))\n",
    "print(\"Matthews correlation coefficient: \" + str(matthews_corrcoef(Y, y_pred)))\n",
    "\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "f = open(\"../../Results2/discriminatorResults.txt\", \"a+\")\n",
    "f.write(\"TP: %d, FP: %d, FN: %d, TN: %d\\n\" % (cm[0][0], cm[0][1], cm[1][0], cm[1][1]))\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42fa4db54138ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f6e6d9c140d7dff"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\generator\"\n",
    "discriminator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\discriminator\"\n",
    "\n",
    "# Save the generator\n",
    "gan.generator.save(generator_save_path)\n",
    "# Save the discriminator\n",
    "gan.discriminator.save(discriminator_save_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe8ec4c379f2369",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6a9e36b9e08ee5f"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_load_path = \"/model/generator\"\n",
    "discriminator_load_path = \"/model/discriminator\"\n",
    "\n",
    "gan.generator = load_model(generator_load_path)\n",
    "gan.discriminator = load_model(discriminator_load_path)\n",
    "\n",
    "gan.generator.summary()\n",
    "gan.discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d47a79ec79641d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5994216370e2cbc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Will continue to run until a better model is found"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c8dc981fd83dae"
  },
  {
   "cell_type": "code",
   "source": [
    "class Looper:\n",
    "    def random_numbers(self):\n",
    "        gen_hidden1 = np.random.randint(1, 101)\n",
    "        gen_hidden2 = np.random.randint(1, 101)\n",
    "        gen_hidden3 = np.random.randint(1, 101)\n",
    "        return [gen_hidden1, gen_hidden2, gen_hidden3]\n",
    "    \n",
    "    def evaluate(gan):\n",
    "        noise = tf.random.normal((num_samples, input_shape))\n",
    "        generated_samples = gan.generator(noise)\n",
    "        discriminator_predictions = gan.discriminator.predict(generated_samples)\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        discriminator_predictions_rounded = np.round(discriminator_predictions).flatten()\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        accuracy = accuracy_score(ideal_output, discriminator_predictions_rounded)\n",
    "        f1 = f1_score(ideal_output, discriminator_predictions_rounded)\n",
    "        return accuracy, f1\n",
    "    \n",
    "    def save(gan):\n",
    "        generator_save_path = \"model/best_generator\"\n",
    "        discriminator_save_path = \"model/best_discriminator\"\n",
    "        gan.generator.save(generator_save_path)\n",
    "        gan.discriminator.save(discriminator_save_path)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5054a1c9e3dbf41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Reuslt From Experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fedb5448092be35"
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "f = open(\"GeneratorHypersAbove50percentAccuracy.txt\", \"w\")\n",
    "f.write(\"\"\"\"\"\" Hidden layer counts for Generator model that resulted in over 50% generated attacks labeled correctly:\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\"\"\")\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 =  np.random.randint(1, 101)\n",
    "    gen_hidden2 =  np.random.randint(1, 101)\n",
    "    gen_hidden3 =  np.random.randint(1, 101)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 100:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"../../Results2/GANresultsportsweep%.0f%.0f%.0fiter%.0ftry2.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"../../Results2/GANresultsportsweep.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated portsweep attacks\n",
    "        portsweep_labels = np.full((len(results),), portsweep_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(portsweep_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"../../Results2/GeneratorHypersAbove50percentAccuracyportsweep.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            result_filename = result_filename\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1\n",
    "            \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3adfbebb93ec143",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "384c6f2478ba0be7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
