{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Running Import Statements and ensuring GPU Support"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16bc02c3fb613f07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:27:37.306217Z",
     "start_time": "2024-04-17T07:27:34.315371Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices and configure them before any other operations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth on the GPU to true\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth set\")\n",
    "            print(\"GPU Device:\", gpu, \"\\n\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before initializing the GPUs\n",
    "        print(\"RuntimeError in setting up GPU:\", e)\n",
    "        \n",
    "    try:\n",
    "        # Optional: Set a memory limit\n",
    "        memory_limit = 8000  # e.g., 4096 MB for 4GB\n",
    "        config = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [config])\n",
    "        print(f\"Memory limit set to {memory_limit}MB on GPU {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory limit: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Print versions and device configurations after ensuring GPU settings\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA version:\", tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(\"cuDNN version:\", tf.sysconfig.get_build_info()['cudnn_version'])\n",
    "print(tf.config.list_physical_devices(), \"\\n\", tf.config.list_logical_devices(), \"\\n\")\n",
    "print(tf.config.list_physical_devices('GPU'), \"\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth set\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \n",
      "\n",
      "Memory limit set to 8000MB on GPU /physical_device:GPU:0\n",
      "TensorFlow version: 2.10.0\n",
      "CUDA version: 64_112\n",
      "cuDNN version: 64_8\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      " [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')] \n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dbd3b980448fb21"
  },
  {
   "cell_type": "code",
   "source": [
    "rel_path = '/archive/'          # If your dataset is within your python project directory, change this to the relative path to your dataset\n",
    "path = os.getcwd() + rel_path   # If your dataset is somewhere else, change this to that path\n",
    "csv_filepaths = glob.glob(os.path.join(path, \"*.csv\"))  # Makes a list of all CSVs within the directory above\n",
    "\n",
    "csv_filepaths = csv_filepaths[:10]\n",
    "\n",
    "# Load the first csv file\n",
    "df = pd.read_csv(csv_filepaths[0])  # astype(column_datatypes)\n",
    "\n",
    "# Load CSV files in batches\n",
    "batch_size = 10\n",
    "df_list = []\n",
    "\n",
    "print(\"Test\")\n",
    "\n",
    "for idx, filepath in enumerate(csv_filepaths):\n",
    "    try:\n",
    "        # Load each CSV with predefined column data types\n",
    "        temp_df = pd.read_csv(filepath)\n",
    "        df_list.append(temp_df)\n",
    "        clear_output(wait=False)\n",
    "        print(f'Loading CSV {idx + 1}/{len(csv_filepaths)}')\n",
    "        \n",
    "        # Optionally, process data in chunks to avoid large memory use at once\n",
    "        if len(df_list) >= batch_size:\n",
    "            if 'df' in locals():\n",
    "                df = pd.concat([df, pd.concat(df_list)], ignore_index=True)\n",
    "            else:\n",
    "                df = pd.concat(df_list, ignore_index=True)\n",
    "            df_list = []  # Reset the batch list to free up memory\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {filepath}: {e}\")\n",
    "\n",
    "# Concatenate any remaining dataframes\n",
    "if df_list:\n",
    "    if 'df' in locals():\n",
    "        df = pd.concat([df, pd.concat(df_list)], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "print(\"All files loaded.\")\n",
    "df.info()  # Show dataframe info to confirm successful load and data types"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6107541190bd38e",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2605643 entries, 0 to 2605642\n",
      "Data columns (total 47 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   flow_duration    float64\n",
      " 1   Header_Length    float64\n",
      " 2   Protocol Type    float64\n",
      " 3   Duration         float64\n",
      " 4   Rate             float64\n",
      " 5   Srate            float64\n",
      " 6   Drate            float64\n",
      " 7   fin_flag_number  float64\n",
      " 8   syn_flag_number  float64\n",
      " 9   rst_flag_number  float64\n",
      " 10  psh_flag_number  float64\n",
      " 11  ack_flag_number  float64\n",
      " 12  ece_flag_number  float64\n",
      " 13  cwr_flag_number  float64\n",
      " 14  ack_count        float64\n",
      " 15  syn_count        float64\n",
      " 16  fin_count        float64\n",
      " 17  urg_count        float64\n",
      " 18  rst_count        float64\n",
      " 19  HTTP             float64\n",
      " 20  HTTPS            float64\n",
      " 21  DNS              float64\n",
      " 22  Telnet           float64\n",
      " 23  SMTP             float64\n",
      " 24  SSH              float64\n",
      " 25  IRC              float64\n",
      " 26  TCP              float64\n",
      " 27  UDP              float64\n",
      " 28  DHCP             float64\n",
      " 29  ARP              float64\n",
      " 30  ICMP             float64\n",
      " 31  IPv              float64\n",
      " 32  LLC              float64\n",
      " 33  Tot sum          float64\n",
      " 34  Min              float64\n",
      " 35  Max              float64\n",
      " 36  AVG              float64\n",
      " 37  Std              float64\n",
      " 38  Tot size         float64\n",
      " 39  IAT              float64\n",
      " 40  Number           float64\n",
      " 41  Magnitue         float64\n",
      " 42  Radius           float64\n",
      " 43  Covariance       float64\n",
      " 44  Variance         float64\n",
      " 45  Weight           float64\n",
      " 46  label            object \n",
      "dtypes: float64(46), object(1)\n",
      "memory usage: 934.3+ MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataframe Memory Size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d196c76ad9f33c54"
  },
  {
   "cell_type": "code",
   "source": [
    "tot_mem = df.memory_usage().sum()\n",
    "print(f'{tot_mem / 1000000000} gb')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:28:18.854114Z",
     "start_time": "2024-04-17T07:28:18.843608Z"
    }
   },
   "id": "22923238d1d94197",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979721896 gb\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64aaa3e725257871"
  },
  {
   "cell_type": "code",
   "source": [
    "label_maps = { 'Backdoor_Malware': 0,         'BenignTraffic': 1,           'BrowserHijacking': 2,\n",
    "               'CommandInjection': 3,         'DDoS-ACK_Fragmentation': 4,  'DDoS-HTTP_Flood': 5,\n",
    "               'DDoS-ICMP_Flood': 6,          'DDoS-ICMP_Fragmentation': 7, 'DDoS-PSHACK_Flood': 8,\n",
    "               'DDoS-RSTFINFlood': 9,         'DDoS-SYN_Flood': 10,         'DDoS-SlowLoris': 11,\n",
    "               'DDoS-SynonymousIP_Flood': 12, 'DDoS-TCP_Flood': 13,         'DDoS-UDP_Flood': 14,\n",
    "               'DDoS-UDP_Fragmentation': 15,  'DNS_Spoofing': 16,           'DictionaryBruteForce': 17,\n",
    "               'DoS-HTTP_Flood': 18,          'DoS-SYN_Flood': 19,          'DoS-TCP_Flood': 20,\n",
    "               'DoS-UDP_Flood': 21,           'MITM-ArpSpoofing': 22,       'Mirai-greeth_flood': 23,\n",
    "               'Mirai-greip_flood': 24,       'Mirai-udpplain': 25,         'Recon-HostDiscovery': 26,\n",
    "               'Recon-OSScan': 27,            'Recon-PingSweep': 28,        'Recon-PortScan': 29,\n",
    "               'SqlInjection': 30,            'Uploading_Attack': 31,       'VulnerabilityScan': 32, \n",
    "               'XSS': 33\n",
    "             }\n",
    "\n",
    "df['label'] = df['label'].map(label_maps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:28:21.437417Z",
     "start_time": "2024-04-17T07:28:21.324343Z"
    }
   },
   "id": "5c4230b84755fee9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a63563bc6cf116"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e2bd6be73552dd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters for the Machine Learning Model or GAN setup\n",
    "\n",
    "# Input shape for the model or the initial layer of the generator\n",
    "input_shape = 46\n",
    "\n",
    "# Training Configuration\n",
    "# num_epochs = 50       # Number of training epochs overall (if applicable)\n",
    "batch_size = 512     # Batch size for training\n",
    "epochs = 7000        # Specific to the generator or another component\n",
    "\n",
    "# GAN-specific Parameters\n",
    "critic_updates = 5   # Number of critic updates per generator update in a GAN\n",
    "\n",
    "# Sampling and Class Configuration\n",
    "num_samples = 10000    # Number of samples to generate or process\n",
    "specific_attack_classes = [0, 1, 2, 3, 4 , 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "num_classes = len(specific_attack_classes)  # Total number of unique classes\n",
    "\n",
    "# Display DataFrame (Optional: you can remove this if it was for a check)\n",
    "result = df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:28:24.921878Z",
     "start_time": "2024-04-17T07:28:24.906858Z"
    }
   },
   "id": "51af0bb4842459d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Towson Normal GAN Structure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b3b7de14e652cc"
  },
  {
   "cell_type": "code",
   "source": [
    "# GAN class\n",
    "# This class contains the generator and discriminator models, as well as the training loop for the GAN\n",
    "class GAN:\n",
    "    def __init__(self, hidden1, hidden2, hidden3, input_shape, num_classes):\n",
    "        # store the parameters as instance variables\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.hidden3 = hidden3\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # build the generator and discriminator\n",
    "        self.generator = self.build_generator(self.hidden1, self.hidden2, self.hidden3, self.input_shape)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # setting the loss function for generator and discriminator\n",
    "        self.optimizer = Adam(0.0002, 0.5)\n",
    "        # self.generator.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "        self.discriminator.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def build_generator(self, hidden1, hidden2, hidden3, input_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden1, input_dim=input_dim))  \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden3))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(input_dim, activation='relu'))  # Changed from output_dim to input_dim\n",
    "\n",
    "        noise = Input(shape=(input_dim,))\n",
    "        attack = model(noise)\n",
    "        return Model(noise, attack)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_shape, input_dim=input_shape, activation='relu'))  \n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(15, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "        attack = Input(shape=(input_shape,))\n",
    "        validity = model(attack)\n",
    "\n",
    "        return Model(attack, validity)\n",
    "    \n",
    "   \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return tf.reduce_mean(fake_output)\n",
    "\n",
    "\n",
    "    def trainGAN(self, gen_hidden1, gen_hidden2, gen_hidden3, input_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param gen_hidden1: \n",
    "        :param gen_hidden2: \n",
    "        :param gen_hidden3: \n",
    "        :param input_dim: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Setting up Optimizer\n",
    "        \"\"\"\n",
    "        # optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        \"\"\"\n",
    "        Getting the data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Directly use 'result' DataFrame. Ensure it's accessible within this scope.\n",
    "        # Sampling 500 data points randomly from 'result'\n",
    "        sampled_df = result.sample(10000).reset_index(drop=True)\n",
    "        \"\"\"\n",
    "        Redundant Label Encoding\n",
    "        \"\"\"\n",
    "        le = LabelEncoder()\n",
    "        sampled_df['label'] = le.fit_transform(sampled_df['label'])\n",
    "        \n",
    "        \"\"\"\n",
    "        Splitting the data into features and labels\n",
    "        \"\"\"\n",
    "        # Split the data into training and testing sets\n",
    "        X = sampled_df.drop('label', axis=1)  # Features\n",
    "        y = sampled_df['label']               # Target label\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Ensure you reset index on X_train if accessing by loc or iloc later\n",
    "        X_train.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Output the memory usage and the shapes of training/testing datasets\n",
    "        print(f'Memory usage: {df.memory_usage().sum() / 1000000000} GB')\n",
    "        print('Training set shape:', X_train.shape)\n",
    "        print('Test set shape:', X_test.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        Setting up labels for valid (real) and fake data for training\n",
    "        \"\"\"\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        \"\"\"\n",
    "        Building the discriminator\n",
    "        \"\"\"\n",
    "        # discriminator = self.build_discriminator()\n",
    "        # discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        \"\"\"\n",
    "        Building the generator\n",
    "        \"\"\"\n",
    "        # generator = self.build_generator(gen_hidden1, gen_hidden2, gen_hidden3, input_dim)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Setting up the combined model\n",
    "        \"\"\"\n",
    "        z = Input(shape=(input_shape,))\n",
    "        attack = self.generator(z)\n",
    "        validity = self.discriminator(attack)\n",
    "        combined = Model(z, validity)\n",
    "        combined.compile(loss='categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \"\"\"\n",
    "        set up of break conditions for training when the generator is worsening\n",
    "        \"\"\"\n",
    "        loss_increase_count = 0\n",
    "        prev_g_loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        TRAINING LOOP\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Get Training Data from X_train\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_attacks = X_train.iloc[idx]\n",
    "            \n",
    "            # Ensure all data is numeric and replace NaNs\n",
    "            real_attacks = real_attacks.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "        \n",
    "            # Convert DataFrame to NumPy array and ensure dtype is float32\n",
    "            real_attacks_np = real_attacks.astype('float32').to_numpy()  # is it normalized?\n",
    "\n",
    "            # Run Generator\n",
    "            noise = tf.random.normal((batch_size, input_shape))  # Make the noise input\n",
    "            gen_attacks = self.generator.predict(noise)  # Make the synthetic data\n",
    "            \n",
    "            # Train Discriminator with training data and synthetic data\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_attacks_np, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_attacks, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator from the noise and valid label by using the combined model to have the generator interact with the discriminator\n",
    "            g_loss = combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            # at the end of 100 epochs print the losses and accuracy\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "            \n",
    "            # if the loss is greater than previous loss then increase the counter \n",
    "            if (g_loss - prev_g_loss) > 0: \n",
    "                loss_increase_count = loss_increase_count + 1\n",
    "            else: \n",
    "                loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "                \n",
    "            prev_g_loss = g_loss\n",
    "            \n",
    "            # Conditions to stop the loop if generator loss increases 5 times    \n",
    "            if loss_increase_count > 5:\n",
    "                print('Stoping on iteration: ', epoch)\n",
    "                break\n",
    "            \n",
    "            # saving the generated output\n",
    "            if epoch % 20 == 0:\n",
    "                f = open(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", \"a\")\n",
    "                np.savetxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "                f.close()\n",
    "\n",
    "        # peek at our results\n",
    "        results = np.loadtxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\")  # save final output\n",
    "        print(\"Generated attacks: \")\n",
    "        print(results[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:28:29.209606Z",
     "start_time": "2024-04-17T07:28:29.195100Z"
    }
   },
   "id": "e440bc51ca32170f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GAN Setup & Training Prep"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26106e7d66f68152"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Randomly select hidden layer sizes for the generator\n",
    "gen_hidden1 = np.random.randint(1, 101)\n",
    "gen_hidden2 = np.random.randint(1, 101)\n",
    "gen_hidden3 = np.random.randint(1, 101)\n",
    "\n",
    "# Create the GAN with the selected hidden layer sizes\n",
    "gan = GAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape, num_classes)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Hidden Layers: \", gen_hidden1, gen_hidden2, gen_hidden3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b6150bc0837309",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layers:  100 24 21\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN GAN Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd6995f4eee9f52c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the trainGAN function directly to start training\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "gan.trainGAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "clear_output(wait=False)\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "print(\"Training Complete in {:.2f} seconds!!!\".format(end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:29:06.991554Z",
     "start_time": "2024-04-17T07:28:37.939825Z"
    }
   },
   "id": "b38ec80b1a28cac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAN with hidden layers:  100 24 21\n",
      "Memory usage: 0.979721896 GB\n",
      "Training set shape: (5000, 46)\n",
      "Test set shape: (5000, 46)\n",
      "16/16 [==============================] - 1s 1ms/step\n",
      "0 [D loss: 0.0, acc.: 5.37109375%] [G loss: 0.0]\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 900us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 801us/step\n",
      "16/16 [==============================] - 0s 946us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "100 [D loss: 0.0, acc.: 50.0%] [G loss: 0.0]\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 967us/step\n",
      "16/16 [==============================] - 0s 900us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 972us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 972us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 941us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 980us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 938us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 976us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "200 [D loss: 0.0, acc.: 50.0%] [G loss: 0.0]\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 991us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 900us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 974us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "300 [D loss: 0.0, acc.: 50.0%] [G loss: 0.0]\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Start the timer\u001B[39;00m\n\u001B[0;32m      5\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 7\u001B[0m \u001B[43mgan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainGAN\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgen_hidden1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_hidden2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_hidden3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     11\u001B[0m clear_output(wait\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[6], line 145\u001B[0m, in \u001B[0;36mGAN.trainGAN\u001B[1;34m(self, gen_hidden1, gen_hidden2, gen_hidden3, input_dim)\u001B[0m\n\u001B[0;32m    142\u001B[0m real_attacks \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39miloc[idx]\n\u001B[0;32m    144\u001B[0m \u001B[38;5;66;03m# Ensure all data is numeric and replace NaNs\u001B[39;00m\n\u001B[1;32m--> 145\u001B[0m real_attacks \u001B[38;5;241m=\u001B[39m \u001B[43mreal_attacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcoerce\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;241m0.0\u001B[39m)\n\u001B[0;32m    147\u001B[0m \u001B[38;5;66;03m# Convert DataFrame to NumPy array and ensure dtype is float32\u001B[39;00m\n\u001B[0;32m    148\u001B[0m real_attacks_np \u001B[38;5;241m=\u001B[39m real_attacks\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto_numpy()  \u001B[38;5;66;03m# is it normalized?\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\frame.py:10361\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m  10347\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m  10349\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m  10350\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m  10351\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m  10359\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m  10360\u001B[0m )\n\u001B[1;32m> 10361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\apply.py:916\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw(engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine, engine_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine_kwargs)\n\u001B[1;32m--> 916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\apply.py:1068\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1065\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_series_numba()\n\u001B[0;32m   1067\u001B[0m \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[1;32m-> 1068\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\apply.py:1107\u001B[0m, in \u001B[0;36mFrameApply.wrap_results\u001B[1;34m(self, results, res_index)\u001B[0m\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;66;03m# see if we can infer the results\u001B[39;00m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(results) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01min\u001B[39;00m results \u001B[38;5;129;01mand\u001B[39;00m is_sequence(results[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m-> 1107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_results_for_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1109\u001B[0m \u001B[38;5;66;03m# dict of scalars\u001B[39;00m\n\u001B[0;32m   1110\u001B[0m \n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# the default dtype of an empty Series is `object`, but this\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m \u001B[38;5;66;03m# code can be hit by df.mean() where the result should have dtype\u001B[39;00m\n\u001B[0;32m   1113\u001B[0m \u001B[38;5;66;03m# float64 even if it's an empty Series.\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m constructor_sliced \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_constructor_sliced\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\apply.py:1219\u001B[0m, in \u001B[0;36mFrameRowApply.wrap_results_for_axis\u001B[1;34m(self, results, res_index)\u001B[0m\n\u001B[0;32m   1216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1219\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_constructor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m   1221\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arrays must be of the same length\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n\u001B[0;32m   1222\u001B[0m         \u001B[38;5;66;03m# e.g. result = [[2, 3], [1.5], ['foo', 'bar']]\u001B[39;00m\n\u001B[0;32m   1223\u001B[0m         \u001B[38;5;66;03m#  see test_agg_listlike_result GH#29587\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\frame.py:767\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    761\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[0;32m    762\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    763\u001B[0m     )\n\u001B[0;32m    765\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    766\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[1;32m--> 767\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    500\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[0;32m    501\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    149\u001B[0m axes \u001B[38;5;241m=\u001B[39m [columns, index]\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_block_manager_from_column_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefs\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2137\u001B[0m, in \u001B[0;36mcreate_block_manager_from_column_arrays\u001B[1;34m(arrays, axes, consolidate, refs)\u001B[0m\n\u001B[0;32m   2119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_block_manager_from_column_arrays\u001B[39m(\n\u001B[0;32m   2120\u001B[0m     arrays: \u001B[38;5;28mlist\u001B[39m[ArrayLike],\n\u001B[0;32m   2121\u001B[0m     axes: \u001B[38;5;28mlist\u001B[39m[Index],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2133\u001B[0m     \u001B[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001B[39;00m\n\u001B[0;32m   2134\u001B[0m     \u001B[38;5;66;03m#  verify_integrity=False below.\u001B[39;00m\n\u001B[0;32m   2136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2137\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m \u001B[43m_form_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2138\u001B[0m         mgr \u001B[38;5;241m=\u001B[39m BlockManager(blocks, axes, verify_integrity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   2139\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2210\u001B[0m, in \u001B[0;36m_form_blocks\u001B[1;34m(arrays, consolidate, refs)\u001B[0m\n\u001B[0;32m   2207\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(dtype\u001B[38;5;241m.\u001B[39mtype, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbytes\u001B[39m)):\n\u001B[0;32m   2208\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;28mobject\u001B[39m)\n\u001B[1;32m-> 2210\u001B[0m values, placement \u001B[38;5;241m=\u001B[39m \u001B[43m_stack_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtup_block\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_dtlike:\n\u001B[0;32m   2212\u001B[0m     values \u001B[38;5;241m=\u001B[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2252\u001B[0m, in \u001B[0;36m_stack_arrays\u001B[1;34m(tuples, dtype)\u001B[0m\n\u001B[0;32m   2250\u001B[0m stacked \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(shape, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m   2251\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, arr \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(arrays):\n\u001B[1;32m-> 2252\u001B[0m     stacked[i] \u001B[38;5;241m=\u001B[39m arr\n\u001B[0;32m   2254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stacked, placement\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f19b5928dd91b7"
  },
  {
   "cell_type": "code",
   "source": [
    "def getAccuracies()  :\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    for i in range(100) :\n",
    "        # Generate samples from the trained generator\n",
    "        noise = tf.random.normal((num_samples, input_shape))\n",
    "        generated_samples = gan.generator(noise)\n",
    "\n",
    "        # Pass the generated samples through the discriminator\n",
    "        discriminator_predictions = gan.discriminator.predict(generated_samples)\n",
    "\n",
    "        # The ideal output for generated samples is 1\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "\n",
    "        # Correcting the prediction rounding\n",
    "        discriminator_predictions_rounded = np.round(discriminator_predictions).flatten()\n",
    "\n",
    "        # Now, calculating the accuracy should not throw an error\n",
    "        accuracy = accuracy_score(ideal_output, discriminator_predictions_rounded)\n",
    "        f1 = f1_score(ideal_output, discriminator_predictions_rounded)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    accuracy = np.mean(accuracy_scores)\n",
    "    f1 = np.mean(f1_scores)\n",
    "    return accuracy, f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6995ec92cf76edf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy,f1 = getAccuracies()\n",
    "\n",
    "clear_output(wait=False)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42fa4db54138ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f6e6d9c140d7dff"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\generator\"\n",
    "discriminator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\discriminator\"\n",
    "\n",
    "# Save the generator\n",
    "gan.generator.save(generator_save_path)\n",
    "# Save the discriminator\n",
    "gan.discriminator.save(discriminator_save_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe8ec4c379f2369",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6a9e36b9e08ee5f"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_load_path = \"/model/generator\"\n",
    "discriminator_load_path = \"/model/discriminator\"\n",
    "\n",
    "gan.generator = load_model(generator_load_path)\n",
    "gan.discriminator = load_model(discriminator_load_path)\n",
    "\n",
    "gan.generator.summary()\n",
    "gan.discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d47a79ec79641d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5994216370e2cbc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Will continue to run until a better model is found"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c8dc981fd83dae"
  },
  {
   "cell_type": "code",
   "source": [
    "class Looper:\n",
    "    def random_numbers(self):\n",
    "        gen_hidden1 = np.random.randint(1, 101)\n",
    "        gen_hidden2 = np.random.randint(1, 101)\n",
    "        gen_hidden3 = np.random.randint(1, 101)\n",
    "        return [gen_hidden1, gen_hidden2, gen_hidden3]\n",
    "    \n",
    "    def evaluate(gan):\n",
    "        noise = tf.random.normal((num_samples, input_shape))\n",
    "        generated_samples = gan.generator(noise)\n",
    "        discriminator_predictions = gan.discriminator.predict(generated_samples)\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        discriminator_predictions_rounded = np.round(discriminator_predictions).flatten()\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        accuracy = accuracy_score(ideal_output, discriminator_predictions_rounded)\n",
    "        f1 = f1_score(ideal_output, discriminator_predictions_rounded)\n",
    "        return accuracy, f1\n",
    "    \n",
    "    def save(gan):\n",
    "        generator_save_path = \"model/best_generator\"\n",
    "        discriminator_save_path = \"model/best_discriminator\"\n",
    "        gan.generator.save(generator_save_path)\n",
    "        gan.discriminator.save(discriminator_save_path)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5054a1c9e3dbf41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Reuslt From Experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fedb5448092be35"
  },
  {
   "cell_type": "code",
   "source": [
    "looper = Looper()\n",
    "\n",
    "if gan is None:\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "else:\n",
    "    best_accuracy, best_f1 = looper.evaluate(gan)\n",
    "\n",
    "while(True):\n",
    "    # Randomly select hidden layer sizes for the generator\n",
    "    [gen_hidden1, gen_hidden2, gen_hidden3] = looper.random_numbers()\n",
    "\n",
    "    # Create the GAN with the selected hidden layer sizes\n",
    "    gan = GAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape, num_classes)\n",
    "    # Call the trainGAN function directly to start training\n",
    "    gan.trainGAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape)\n",
    "    accuracy, f1 = getAccuracies(gan)   \n",
    "    print(\"Accuracy: \", accuracy, \"F1 Score: \", f1, \"Hidden Layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_f1 = f1\n",
    "        Looper.save(gan)\n",
    "        print(\"Saved New Model\")\n",
    "        break\n",
    "    \n",
    "\n",
    "print(\"Accuracy: \", best_accuracy, \"F1 Score: \", best_f1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3adfbebb93ec143",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "384c6f2478ba0be7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
