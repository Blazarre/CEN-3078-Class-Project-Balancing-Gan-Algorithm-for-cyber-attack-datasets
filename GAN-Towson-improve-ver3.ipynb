{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Running Import Statements and ensuring GPU Support"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16bc02c3fb613f07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:23.251698Z",
     "start_time": "2024-04-19T07:08:19.549249Z"
    }
   },
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices and configure them before any other operations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth on the GPU to true\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth set\")\n",
    "            print(\"GPU Device:\", gpu, \"\\n\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before initializing the GPUs\n",
    "        print(\"RuntimeError in setting up GPU:\", e)\n",
    "        \n",
    "    try:\n",
    "        # Optional: Set a memory limit\n",
    "        memory_limit = 8000  # e.g., 4096 MB for 4GB\n",
    "        config = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [config])\n",
    "        print(f\"Memory limit set to {memory_limit}MB on GPU {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory limit: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Print versions and device configurations after ensuring GPU settings\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA version:\", tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(\"cuDNN version:\", tf.sysconfig.get_build_info()['cudnn_version'])\n",
    "print(tf.config.list_physical_devices(), \"\\n\", tf.config.list_logical_devices(), \"\\n\")\n",
    "print(tf.config.list_physical_devices('GPU'), \"\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth set\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \n",
      "\n",
      "Memory limit set to 8000MB on GPU /physical_device:GPU:0\n",
      "TensorFlow version: 2.10.0\n",
      "CUDA version: 64_112\n",
      "cuDNN version: 64_8\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      " [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')] \n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Dataset",
   "id": "31ee08542e6ad9e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:25.942670Z",
     "start_time": "2024-04-19T07:08:25.938653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_DIRECTORY = './archive/'          # If your dataset is within your python project directory, change this to the relative path to your dataset\n",
    "csv_filepaths = [filename for filename in os.listdir(DATASET_DIRECTORY) if filename.endswith('.csv')]\n",
    "\n",
    "print(csv_filepaths)"
   ],
   "id": "539315e1c513404b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00060-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00061-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00062-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00063-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00064-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00065-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00066-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00067-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00068-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00069-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00070-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00071-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00072-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00073-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00074-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00075-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00076-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00077-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00078-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00079-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00080-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00081-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00082-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00083-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00084-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00085-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00086-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00087-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00088-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00089-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00090-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00091-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00092-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00093-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00094-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00095-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00096-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00097-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00098-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00099-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00100-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00101-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00102-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00103-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00104-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00106-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00107-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00108-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00109-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00110-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00111-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00113-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00115-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00116-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00117-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00119-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00120-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00122-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00123-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00124-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00125-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00126-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00127-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00128-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00129-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00130-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00131-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00132-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00133-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00134-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00135-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00138-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00139-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00140-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00141-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00142-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00143-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00144-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00145-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00146-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00147-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00148-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00149-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00150-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00151-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00152-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00153-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00154-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00155-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00156-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00157-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00158-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00159-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00160-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00161-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00162-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00163-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00164-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00165-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00166-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00168-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "### Loading Dataset",
   "metadata": {
    "collapsed": false
   },
   "id": "6dbd3b980448fb21"
  },
  {
   "cell_type": "code",
   "source": [
    "csv_filepaths = [filename for filename in os.listdir(DATASET_DIRECTORY) if filename.endswith('.csv')]\n",
    "\n",
    "# If there are more than 40 CSV files, randomly select 40 files from the list\n",
    "sample_size = 4\n",
    "\n",
    "if len(csv_filepaths) > sample_size:\n",
    "    csv_filepaths = random.sample(csv_filepaths, sample_size)\n",
    "    print(csv_filepaths)\n",
    "\n",
    "csv_filepaths.sort()\n",
    "\n",
    "# Calculate the index for splitting the datasets into training (80%) and testing (20%)\n",
    "split_index = int(len(csv_filepaths) * 0.8)\n",
    "\n",
    "training_sets = csv_filepaths[:split_index]\n",
    "test_sets = csv_filepaths[split_index:]\n",
    "\n",
    "print(\"Training Sets:\\n\",training_sets, \"\\n\")\n",
    "print(\"Test Sets:\\n\",test_sets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:30.033168Z",
     "start_time": "2024-04-19T07:08:30.018611Z"
    }
   },
   "id": "a6107541190bd38e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n",
      "Training Sets:\n",
      " ['part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv'] \n",
      "\n",
      "Test Sets:\n",
      " ['part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Labeling Features and Labels",
   "id": "8d015a262bf3d40"
  },
  {
   "cell_type": "code",
   "source": [
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:33.828334Z",
     "start_time": "2024-04-19T07:08:33.814516Z"
    }
   },
   "id": "22923238d1d94197",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scaling",
   "id": "d9368c91949598c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:38.025551Z",
     "start_time": "2024-04-19T07:08:38.014257Z"
    }
   },
   "cell_type": "code",
   "source": "scaler = StandardScaler()",
   "id": "4649c65a56bda03",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:42.920183Z",
     "start_time": "2024-04-19T07:08:40.715422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for train_set in tqdm(training_sets):\n",
    "    scaler.fit(pd.read_csv(DATASET_DIRECTORY + train_set)[X_columns])"
   ],
   "id": "33e610b644e9a1dd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "# Classifications",
   "metadata": {
    "collapsed": false
   },
   "id": "64aaa3e725257871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification: 8 (7+1) classes",
   "id": "d6d06a4088dd1c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T13:19:04.870182Z",
     "start_time": "2024-04-18T13:19:04.861598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict_7classes = {}\n",
    "dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "\n",
    "dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "\n",
    "\n",
    "dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "\n",
    "dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "\n",
    "dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "\n",
    "dict_7classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_7classes['BrowserHijacking'] = 'Web'\n",
    "dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "dict_7classes['XSS'] = 'Web'\n",
    "dict_7classes['Uploading_Attack'] = 'Web'\n",
    "dict_7classes['SqlInjection'] = 'Web'\n",
    "dict_7classes['CommandInjection'] = 'Web'\n",
    "\n",
    "\n",
    "dict_7classes['DictionaryBruteForce'] = 'BruteForce'"
   ],
   "id": "b693750ac071637f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification: 2 (1+1) Classes",
   "id": "e2d4e6a311cbe380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:51.572770Z",
     "start_time": "2024-04-19T07:08:51.560270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict_2classes = {}\n",
    "dict_2classes['DDoS-RSTFINFlood'] = 'Attack'\n",
    "dict_2classes['DDoS-PSHACK_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SynonymousIP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ACK_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-SlowLoris'] = 'Attack'\n",
    "dict_2classes['DDoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "dict_2classes['DoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "\n",
    "dict_2classes['Mirai-greeth_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-greip_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-udpplain'] = 'Attack'\n",
    "\n",
    "dict_2classes['Recon-PingSweep'] = 'Attack'\n",
    "dict_2classes['Recon-OSScan'] = 'Attack'\n",
    "dict_2classes['Recon-PortScan'] = 'Attack'\n",
    "dict_2classes['VulnerabilityScan'] = 'Attack'\n",
    "dict_2classes['Recon-HostDiscovery'] = 'Attack'\n",
    "\n",
    "dict_2classes['DNS_Spoofing'] = 'Attack'\n",
    "dict_2classes['MITM-ArpSpoofing'] = 'Attack'\n",
    "\n",
    "dict_2classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_2classes['BrowserHijacking'] = 'Attack'\n",
    "dict_2classes['Backdoor_Malware'] = 'Attack'\n",
    "dict_2classes['XSS'] = 'Attack'\n",
    "dict_2classes['Uploading_Attack'] = 'Attack'\n",
    "dict_2classes['SqlInjection'] = 'Attack'\n",
    "dict_2classes['CommandInjection'] = 'Attack'\n",
    "\n",
    "dict_2classes['DictionaryBruteForce'] = 'Attack'"
   ],
   "id": "7782f0784f9a666d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom Classification",
   "id": "63be9e76674fe225"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# label_categories = {\n",
    "#     'DDOS': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "#     'DOS': [18, 19, 20, 21],\n",
    "#     'Mirai':[23,24,25],\n",
    "#     'Spoofing':[16, 22],\n",
    "#     'Recon': [0,26, 27, 28, 29, 32],\n",
    "#     'Web':[0, 2, 30, 31, 33],\n",
    "#     'BruteForce':[17],\n",
    "#     'Benign':[1]\n",
    "# }\n",
    "# \n",
    "# include_label_categories = []\n",
    "# include_labels = []\n",
    "# \n",
    "# for category in include_label_categories:\n",
    "#     for label in label_categories[category]:\n",
    "#         include_labels.append(label)\n",
    "# \n",
    "# excluded_records = df[~df['labels'].isin(include_labels)]\n",
    "# df = df[df['labels'].isin(include_labels)]\n",
    "# \n",
    "# df"
   ],
   "id": "487488055587c801",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a63563bc6cf116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:08:58.757717Z",
     "start_time": "2024-04-19T07:08:56.138515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "preload the data\n",
    "\"\"\"\n",
    "full_data = pd.DataFrame()\n",
    "for train_set in training_sets:\n",
    "    print(f\"Training set {train_set} out of {len(training_sets)} \\n\")\n",
    "    data_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
    "    df = pd.read_csv(data_path)\n",
    "    full_data = pd.concat([full_data, df])\n",
    "\n",
    "# Shuffle data\n",
    "full_data = shuffle(full_data, random_state=1)\n",
    "\n",
    "# Scale the features\n",
    "full_data[X_columns] = scaler.transform(full_data[X_columns])\n",
    "\n",
    "# Convert DataFrame to NumPy array to facilitate batch operations\n",
    "X_train = full_data[X_columns].values\n",
    "y_train = full_data[y_column].values\n",
    "\n",
    "\"\"\"\n",
    "Redundant Label Encoding\n",
    "\"\"\"\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "print(\"Data fully processed, shapes:\", X_train.shape, y_train.shape)"
   ],
   "id": "a2280f151b373850",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "[ 8 20 21 ...  8 14 20]\n",
      "Data fully processed, shapes: (718204, 46) (718204,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Towson Normal GAN Structure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b3b7de14e652cc"
  },
  {
   "cell_type": "code",
   "source": [
    "# GAN class\n",
    "# This class contains the generator and discriminator models, as well as the training loop for the GAN\n",
    "class GAN:\n",
    "    def __init__(self, hidden1, hidden2, hidden3, input_shape):\n",
    "        # store the parameters as instance variables\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.hidden3 = hidden3\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        # build the generator and discriminator\n",
    "        self.generator = self.build_generator(self.hidden1, self.hidden2, self.hidden3, self.input_shape)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # setting the loss function for generator and discriminator\n",
    "        self.optimizer = RMSprop(lr=0.00005)\n",
    "        # self.generator.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "        self.discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def build_generator(self, hidden1, hidden2, hidden3, input_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden1, input_dim=input_dim))  \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(hidden3))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(input_dim, activation='relu'))  # Changed from output_dim to input_dim\n",
    "\n",
    "        noise = Input(shape=(input_dim,))\n",
    "        attack = model(noise)\n",
    "        return Model(noise, attack)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_shape, input_dim=input_shape, activation='relu'))  \n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(15, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "        attack = Input(shape=(input_shape,))\n",
    "        validity = model(attack)\n",
    "\n",
    "        return Model(attack, validity)\n",
    "    \n",
    "    # def build_combinedModel(self):\n",
    "        \n",
    "    \n",
    "   \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return tf.reduce_mean(fake_output)\n",
    "\n",
    "\n",
    "    def trainGAN(self, training_sets, DATASET_DIRECTORY, batch_size, epochs):\n",
    "        \"\"\"\n",
    "        Train the GAN using data from CSV files.\n",
    "\n",
    "        :param training_sets: List of training dataset filenames.\n",
    "        :param DATASET_DIRECTORY: Directory where datasets are stored.\n",
    "        :param batch_size: Batch size for training.\n",
    "        :param epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        print('Start Training...')\n",
    "        \n",
    "        \"\"\"\n",
    "        Set up of break conditions for training when the generator is worsening\n",
    "        \"\"\"\n",
    "        loss_increase_count = 0\n",
    "        prev_g_loss = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        Labels for real and fake data\n",
    "        \"\"\"\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        \"\"\"\n",
    "        Setting up the combined model\n",
    "        \"\"\"\n",
    "        z = Input(shape=(input_shape,))\n",
    "        attack = self.generator(z)\n",
    "        validity = self.discriminator(attack)\n",
    "        combined = Model(z, validity)\n",
    "        combined.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        print(\"Combined model compiled...\")\n",
    "        \n",
    "        print(\"Training Loop Start...\")\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                # Ensuring the batch is complete\n",
    "                if i + batch_size > X_train.shape[0]:\n",
    "                    continue\n",
    "\n",
    "                # Sample batch data\n",
    "                real_attacks = X_train[i:i+batch_size]\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.input_shape))\n",
    "\n",
    "                # Generate fake data\n",
    "                gen_attacks = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as 1 and fakes as 0)\n",
    "                d_loss_real = self.discriminator.train_on_batch(real_attacks, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_attacks, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # Train the generator (tries to fool the discriminator)\n",
    "                g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Optionally print the progress\n",
    "                if (epoch % 10 == 0) and (i == 0):\n",
    "                    print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "            \n",
    "                # if the loss is greater than previous loss then increase the counter \n",
    "                if (g_loss - prev_g_loss) > 0: \n",
    "                    loss_increase_count = loss_increase_count + 1\n",
    "                else: \n",
    "                    loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "                    \n",
    "                prev_g_loss = g_loss\n",
    "                \n",
    "                # Conditions to stop the loop if generator loss increases 5 times    \n",
    "                if loss_increase_count > 5:\n",
    "                    print('Stoping on iteration: ', epoch)\n",
    "                    break\n",
    "            \n",
    "            # saving the generated output\n",
    "            if epoch % 20 == 0:\n",
    "                f = open(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", \"a\")\n",
    "                np.savetxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "                f.close()\n",
    "\n",
    "        # peek at our results\n",
    "        results = np.loadtxt(\"C:/Users/kskos/PycharmProjects/CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets/Results/GeneratedAttackResults.txt\")  # save final output\n",
    "        print(\"Generated attacks: \")\n",
    "        print(results[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:41:57.217062Z",
     "start_time": "2024-04-18T13:41:57.198530Z"
    }
   },
   "id": "e440bc51ca32170f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "### GAN Setup & Hyper Parameters",
   "metadata": {
    "collapsed": false
   },
   "id": "26106e7d66f68152"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters for the Machine Learning Model or GAN setup\n",
    "\n",
    "# Input shape for the model or the initial layer of the generator\n",
    "input_shape = 46\n",
    "\n",
    "# Training Configuration\n",
    "batch_size = 512     # Batch size for training\n",
    "epochs = 7000        # Specific to the generator or another component\n",
    "\n",
    "# Sampling and Class Configuration\n",
    "num_samples = 10000    # Number of samples to generate or process\n",
    "\n",
    "# Display DataFrame (Optional: you can remove this if it was for a check)\n",
    "\n",
    "# Randomly select hidden layer sizes for the generator\n",
    "gen_hidden1 = 32\n",
    "gen_hidden2 = 16\n",
    "gen_hidden3 = 8\n",
    "\n",
    "# Create the GAN with the selected hidden layer sizes\n",
    "gan = GAN(gen_hidden1, gen_hidden2, gen_hidden3, input_shape)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Hidden Layers: \", gen_hidden1, gen_hidden2, gen_hidden3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b6150bc0837309",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layers:  32 16 8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN GAN Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd6995f4eee9f52c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the trainGAN function directly to start training\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "gan.trainGAN(training_sets, DATASET_DIRECTORY, batch_size, epochs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "clear_output(wait=False)\n",
    "print(\"Training GAN with hidden layers: \", gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "print(\"Training Complete in {:.2f} seconds!!!\".format(end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:42:21.377684Z",
     "start_time": "2024-04-18T13:42:02.928913Z"
    }
   },
   "id": "b38ec80b1a28cac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAN with hidden layers:  32 16 8\n",
      "Start Training...\n",
      "Training set part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00162-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "[13 13  9 ...  6 14  1]\n",
      "Data fully processed, shapes: (905686, 46) (905686,)\n",
      "Combined model compiled...\n",
      "Training Loop Start...\n",
      "16/16 [==============================] - 1s 834us/step\n",
      "Epoch 0/7000 [D loss: 0.0, acc.: 56.45%] [G loss: 0.8089]\n",
      "16/16 [==============================] - 0s 901us/step\n",
      "16/16 [==============================] - 0s 967us/step\n",
      "16/16 [==============================] - 0s 905us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 901us/step\n",
      "Stoping on iteration:  0\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  1\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  2\n",
      "16/16 [==============================] - 0s 935us/step\n",
      "Stoping on iteration:  3\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Stoping on iteration:  4\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  5\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  6\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "Stoping on iteration:  7\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  8\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  9\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Epoch 10/7000 [D loss: 0.0, acc.: 50.20%] [G loss: 0.9510]\n",
      "Stoping on iteration:  10\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  11\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Stoping on iteration:  12\n",
      "16/16 [==============================] - 0s 901us/step\n",
      "Stoping on iteration:  13\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  14\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  15\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  16\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  17\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  18\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  19\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 20/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 1.1501]\n",
      "Stoping on iteration:  20\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  21\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  22\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  23\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  24\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  25\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  26\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "Stoping on iteration:  27\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  28\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  29\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 30/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 1.4484]\n",
      "Stoping on iteration:  30\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Stoping on iteration:  31\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  32\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  33\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  34\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  35\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  36\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  37\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  38\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  39\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 40/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 1.8428]\n",
      "Stoping on iteration:  40\n",
      "16/16 [==============================] - 0s 935us/step\n",
      "Stoping on iteration:  41\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  42\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  43\n",
      "16/16 [==============================] - 0s 935us/step\n",
      "Stoping on iteration:  44\n",
      "16/16 [==============================] - 0s 868us/step\n",
      "Stoping on iteration:  45\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  46\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  47\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  48\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  49\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Epoch 50/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 2.2190]\n",
      "Stoping on iteration:  50\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "Stoping on iteration:  51\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  52\n",
      "16/16 [==============================] - 0s 867us/step\n",
      "Stoping on iteration:  53\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  54\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  55\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  56\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  57\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  58\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  59\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 60/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 2.9737]\n",
      "Stoping on iteration:  60\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Stoping on iteration:  61\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Stoping on iteration:  62\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  63\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  64\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  65\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  66\n",
      "16/16 [==============================] - 0s 867us/step\n",
      "Stoping on iteration:  67\n",
      "16/16 [==============================] - 0s 901us/step\n",
      "Stoping on iteration:  68\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  69\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 70/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 3.4083]\n",
      "Stoping on iteration:  70\n",
      "16/16 [==============================] - 0s 935us/step\n",
      "Stoping on iteration:  71\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  72\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  73\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  74\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  75\n",
      "16/16 [==============================] - 0s 834us/step\n",
      "Stoping on iteration:  76\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  77\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  78\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  79\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 80/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 3.9823]\n",
      "Stoping on iteration:  80\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  81\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  82\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  83\n",
      "16/16 [==============================] - 0s 801us/step\n",
      "Stoping on iteration:  84\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Stoping on iteration:  85\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  86\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  87\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  88\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  89\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch 90/7000 [D loss: 0.0, acc.: 50.00%] [G loss: 4.6961]\n",
      "Stoping on iteration:  90\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  91\n",
      "16/16 [==============================] - 0s 901us/step\n",
      "Stoping on iteration:  92\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  93\n",
      "16/16 [==============================] - 0s 969us/step\n",
      "Stoping on iteration:  94\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  95\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  96\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  97\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Stoping on iteration:  98\n",
      "16/16 [==============================] - 0s 934us/step\n",
      "Stoping on iteration:  99\n",
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Start the timer\u001B[39;00m\n\u001B[0;32m      5\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 7\u001B[0m \u001B[43mgan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainGAN\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_sets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDATASET_DIRECTORY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     11\u001B[0m clear_output(wait\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[7], line 138\u001B[0m, in \u001B[0;36mGAN.trainGAN\u001B[1;34m(self, training_sets, DATASET_DIRECTORY, batch_size, epochs)\u001B[0m\n\u001B[0;32m    135\u001B[0m gen_attacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator\u001B[38;5;241m.\u001B[39mpredict(noise)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;66;03m# Train the discriminator (real classified as 1 and fakes as 0)\u001B[39;00m\n\u001B[1;32m--> 138\u001B[0m d_loss_real \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiscriminator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreal_attacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m d_loss_fake \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator\u001B[38;5;241m.\u001B[39mtrain_on_batch(gen_attacks, fake)\n\u001B[0;32m    140\u001B[0m d_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39madd(d_loss_real, d_loss_fake)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\keras\\engine\\training.py:2381\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001B[0m\n\u001B[0;32m   2377\u001B[0m     iterator \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39msingle_batch_iterator(\n\u001B[0;32m   2378\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001B[0;32m   2379\u001B[0m     )\n\u001B[0;32m   2380\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_train_function()\n\u001B[1;32m-> 2381\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2383\u001B[0m logs \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39msync_to_numpy_or_python_type(logs)\n\u001B[0;32m   2384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RyanEnviornment\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f19b5928dd91b7"
  },
  {
   "cell_type": "code",
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    # define baseline model\n",
    "    def baseline_model(self, num_of_classes):\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        \n",
    "        inputs = 46\n",
    "        hidden_layer1 = 10\n",
    "        hidden_layer2 = 5\n",
    "        hidden_layer3 = 0\n",
    "        outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "        \n",
    "        model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "        if hidden_layer2 != 0:\n",
    "            model.add(Dense(hidden_layer2, activation='relu'))\n",
    "        if hidden_layer3 != 0:\n",
    "            model.add(Dense(hidden_layer3, activation='relu'))\n",
    "        model.add(Dense(outputs, activation='softmax'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #optimizer=adam\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6995ec92cf76edf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#for i in range(0,10):\n",
    "\n",
    "# set up seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# import test data\n",
    "X = pd.read_csv('X_test.csv')  # Load X_test data from CSV\n",
    "Y = pd.read_csv('y_test.csv')  # Load y_test data from CSV\n",
    "\n",
    "# set up for Y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# set up evaluator \n",
    "evaluatorModel = Evaluator()\n",
    "\n",
    "estimator = KerasClassifier(build_fn=evaluatorModel.baseline_model(34), epochs=32, batch_size=200, verbose=2)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "y_pred = cross_val_predict(estimator, X, dummy_y, cv=kfold)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "\n",
    "trained_classifier = estimator.fit(X, Y)\n",
    "print(type(estimator))\n",
    "\n",
    "cm = confusion_matrix(Y, y_pred)\n",
    "print(cm)\n",
    "print(\"total: \" + str(cm.sum()))\n",
    "print(\"accuracy: \" + str(np.trace(cm) / cm.sum()))\n",
    "print(\"Matthews correlation coefficient: \" + str(matthews_corrcoef(Y, y_pred)))\n",
    "\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "f = open(\"../../Results2/discriminatorResults.txt\", \"a+\")\n",
    "f.write(\"TP: %d, FP: %d, FN: %d, TN: %d\\n\" % (cm[0][0], cm[0][1], cm[1][0], cm[1][1]))\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42fa4db54138ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f6e6d9c140d7dff"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\generator\"\n",
    "discriminator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\discriminator\"\n",
    "\n",
    "# Save the generator\n",
    "gan.generator.save(generator_save_path)\n",
    "# Save the discriminator\n",
    "gan.discriminator.save(discriminator_save_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe8ec4c379f2369",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6a9e36b9e08ee5f"
  },
  {
   "cell_type": "code",
   "source": [
    "generator_load_path = \"/model/generator\"\n",
    "discriminator_load_path = \"/model/discriminator\"\n",
    "\n",
    "gan.generator = load_model(generator_load_path)\n",
    "gan.discriminator = load_model(discriminator_load_path)\n",
    "\n",
    "gan.generator.summary()\n",
    "gan.discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d47a79ec79641d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5994216370e2cbc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Will continue to run until a better model is found"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c8dc981fd83dae"
  },
  {
   "cell_type": "code",
   "source": [
    "class Looper:\n",
    "    def random_numbers(self):\n",
    "        gen_hidden1 = np.random.randint(1, 101)\n",
    "        gen_hidden2 = np.random.randint(1, 101)\n",
    "        gen_hidden3 = np.random.randint(1, 101)\n",
    "        return [gen_hidden1, gen_hidden2, gen_hidden3]\n",
    "    \n",
    "    def evaluate(gan):\n",
    "        noise = tf.random.normal((num_samples, input_shape))\n",
    "        generated_samples = gan.generator(noise)\n",
    "        discriminator_predictions = gan.discriminator.predict(generated_samples)\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        discriminator_predictions_rounded = np.round(discriminator_predictions).flatten()\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        accuracy = accuracy_score(ideal_output, discriminator_predictions_rounded)\n",
    "        f1 = f1_score(ideal_output, discriminator_predictions_rounded)\n",
    "        return accuracy, f1\n",
    "    \n",
    "    def save(gan):\n",
    "        generator_save_path = \"model/best_generator\"\n",
    "        discriminator_save_path = \"model/best_discriminator\"\n",
    "        gan.generator.save(generator_save_path)\n",
    "        gan.discriminator.save(discriminator_save_path)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5054a1c9e3dbf41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Reuslt From Experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fedb5448092be35"
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "f = open(\"GeneratorHypersAbove50percentAccuracy.txt\", \"w\")\n",
    "f.write(\"\"\"\"\"\" Hidden layer counts for Generator model that resulted in over 50% generated attacks labeled correctly:\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\"\"\")\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 =  np.random.randint(1, 101)\n",
    "    gen_hidden2 =  np.random.randint(1, 101)\n",
    "    gen_hidden3 =  np.random.randint(1, 101)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 100:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"../../Results2/GANresultsportsweep%.0f%.0f%.0fiter%.0ftry2.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"../../Results2/GANresultsportsweep.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated portsweep attacks\n",
    "        portsweep_labels = np.full((len(results),), portsweep_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(portsweep_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"../../Results2/GeneratorHypersAbove50percentAccuracyportsweep.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            result_filename = result_filename\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1\n",
    "            \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3adfbebb93ec143",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "384c6f2478ba0be7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
